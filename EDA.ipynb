{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "212db142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5526e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape: (630000, 13)\n",
      "test.shape: (270000, 12)\n",
      "orig.shape: (20000, 13)\n",
      "11 Base Features:['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, gc\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "orig = pd.read_csv('Exam_Score_Prediction.csv')\n",
    "\n",
    "print(\"train_shape:\",train.shape)\n",
    "print(\"test.shape:\",test.shape)\n",
    "print(\"orig.shape:\",orig.shape)\n",
    "\n",
    "orig\n",
    "\n",
    "# 今後のためにリストを作る\n",
    "target = 'exam_score'\n",
    "base = [col for col in train.columns if col not in ['id', target]]\n",
    "categories = train.select_dtypes('object').columns.to_list()\n",
    "nums = [col for col in base if col not in categories]\n",
    "print(f'{len(base)} Base Features:{base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5970102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ORIG Features Created.\n"
     ]
    }
   ],
   "source": [
    "ORIG = []\n",
    "\n",
    "# 外部データの各カラムのユニークごとの平均値というカラムを追加する。\n",
    "for col in base:\n",
    "    # 一つの列に対してgroupbyで固有の値をまとめる。それらのtargetをそれぞれ平均する\n",
    "    mean_map = orig.groupby(col)[target].mean() \n",
    "    new_mean_col_name = f\"orig_mean_{col}\"\n",
    "    mean_map.name = new_mean_col_name\n",
    "    \n",
    "    train = train.merge(mean_map, on=col, how='left') # colをキーにして\n",
    "    test = test.merge(mean_map, on=col, how='left')\n",
    "    ORIG.append(new_mean_col_name)\n",
    "    \n",
    "# 外部データの各カラムのユニークごとのサイズというカラムを追加する。\n",
    "    new_count_col_name = f\"orig_count_{col}\"\n",
    "    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n",
    "    \n",
    "    train = train.merge(count_map, on=col, how='left')\n",
    "    test = test.merge(count_map, on=col, how='left')\n",
    "    ORIG.append(new_count_col_name)\n",
    "\n",
    "print(f'{len(ORIG)} ORIG Features Created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8490d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origには存在するが、trainには存在しないカテゴリを全体平均で埋める\n",
    "for col in ORIG:\n",
    "    if 'mean' in col:\n",
    "        train[col] = train[col].fillna(orig[target].mean())\n",
    "        test[col] = test[col].fillna(orig[target].mean())\n",
    "    else:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3406c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_mem_usage はここに定義（そのままでOK）\n",
    "\n",
    "features = base + ORIG\n",
    "\n",
    "# まず X, y を作る（これが先）\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "\n",
    "# test側も、モデルに入れる列だけにそろえる（重要）\n",
    "X_test = test[features].copy()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba7f5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding applied to 6 features.\n",
      "TE_COLS: ['gender', 'course', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n",
      "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance',\n",
      "       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n",
      "       'facility_rating', 'exam_difficulty', 'orig_mean_age', 'orig_count_age',\n",
      "       'orig_mean_gender', 'orig_count_gender', 'orig_mean_course',\n",
      "       'orig_count_course', 'orig_mean_study_hours', 'orig_count_study_hours',\n",
      "       'orig_mean_class_attendance', 'orig_count_class_attendance',\n",
      "       'orig_mean_internet_access', 'orig_count_internet_access',\n",
      "       'orig_mean_sleep_hours', 'orig_count_sleep_hours',\n",
      "       'orig_mean_sleep_quality', 'orig_count_sleep_quality',\n",
      "       'orig_mean_study_method', 'orig_count_study_method',\n",
      "       'orig_mean_facility_rating', 'orig_count_facility_rating',\n",
      "       'orig_mean_exam_difficulty', 'orig_count_exam_difficulty', 'te_gender',\n",
      "       'te_course', 'te_sleep_quality', 'te_study_method',\n",
      "       'te_facility_rating', 'te_exam_difficulty'],\n",
      "      dtype='object')\n",
      "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance',\n",
      "       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n",
      "       'facility_rating', 'exam_difficulty', 'orig_mean_age', 'orig_count_age',\n",
      "       'orig_mean_gender', 'orig_count_gender', 'orig_mean_course',\n",
      "       'orig_count_course', 'orig_mean_study_hours', 'orig_count_study_hours',\n",
      "       'orig_mean_class_attendance', 'orig_count_class_attendance',\n",
      "       'orig_mean_internet_access', 'orig_count_internet_access',\n",
      "       'orig_mean_sleep_hours', 'orig_count_sleep_hours',\n",
      "       'orig_mean_sleep_quality', 'orig_count_sleep_quality',\n",
      "       'orig_mean_study_method', 'orig_count_study_method',\n",
      "       'orig_mean_facility_rating', 'orig_count_facility_rating',\n",
      "       'orig_mean_exam_difficulty', 'orig_count_exam_difficulty', 'te_gender',\n",
      "       'te_course', 'te_sleep_quality', 'te_study_method',\n",
      "       'te_facility_rating', 'te_exam_difficulty'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Target Encoding (OOFでリーク防止) + 列選別条件つき\n",
    "# =========================\n",
    "\n",
    "def select_te_cols(\n",
    "    df_train, df_test, cols,\n",
    "    min_unique=3,              # unique <=2 は除外\n",
    "    max_unique_abs=5000,       # 高カーディナリティ除外\n",
    "    max_unique_ratio=0.30,     # unique/行数 が大きすぎる列は除外（ID化）\n",
    "    max_missing=0.60,          # 欠損率が高い列は除外\n",
    "    rare_thr=5,                # レア判定（出現回数<=5）\n",
    "    max_rare_points_ratio=0.80,# レアカテゴリが占める割合が大きい列は除外\n",
    "    max_unseen_ratio=0.20      # testにしかないカテゴリが多い列は除外\n",
    "):\n",
    "    n = len(df_train)\n",
    "    chosen = []\n",
    "    for col in cols:\n",
    "        s_tr = df_train[col]\n",
    "        s_te = df_test[col]\n",
    "\n",
    "        # 欠損\n",
    "        if s_tr.isna().mean() > max_missing:\n",
    "            continue\n",
    "\n",
    "        # unique\n",
    "        nunq = s_tr.nunique(dropna=True)\n",
    "        if nunq < min_unique:\n",
    "            continue\n",
    "        if nunq > max_unique_abs:\n",
    "            continue\n",
    "        if nunq / n > max_unique_ratio:\n",
    "            continue\n",
    "\n",
    "        # レアカテゴリ比率\n",
    "        vc = s_tr.value_counts(dropna=True)\n",
    "        rare_points_ratio = (s_tr.map(vc).fillna(0) <= rare_thr).mean()\n",
    "        if rare_points_ratio > max_rare_points_ratio:\n",
    "            continue\n",
    "\n",
    "        # unseen比率（testにあるがtrainにないカテゴリの比率）\n",
    "        tr_set = set(s_tr.dropna().unique())\n",
    "        te_set = set(s_te.dropna().unique())\n",
    "        if len(te_set) > 0:\n",
    "            unseen_ratio = len(te_set - tr_set) / len(te_set)\n",
    "            if unseen_ratio > max_unseen_ratio:\n",
    "                continue\n",
    "\n",
    "        chosen.append(col)\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def add_target_encoding_oof(train_df, test_df, y, te_cols, n_splits=5, seed=42, smoothing=20):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for col in te_cols:\n",
    "        te_name = f\"te_{col}\"\n",
    "        train_te = np.zeros(len(train_df), dtype=np.float64)\n",
    "        test_te_folds = []\n",
    "\n",
    "        for tr_idx, va_idx in kf.split(train_df):\n",
    "            X_tr = train_df.iloc[tr_idx]\n",
    "            y_tr = y.iloc[tr_idx]\n",
    "            X_va = train_df.iloc[va_idx]\n",
    "\n",
    "            prior = y_tr.mean()\n",
    "\n",
    "            stats = (\n",
    "                pd.DataFrame({col: X_tr[col].values, \"y\": y_tr.values})\n",
    "                .groupby(col)[\"y\"]\n",
    "                .agg([\"mean\", \"count\"])\n",
    "            )\n",
    "\n",
    "            smooth_map = (stats[\"count\"] * stats[\"mean\"] + smoothing * prior) / (stats[\"count\"] + smoothing)\n",
    "\n",
    "            train_te[va_idx] = X_va[col].map(smooth_map).fillna(prior).astype(np.float64).values\n",
    "            test_te_folds.append(test_df[col].map(smooth_map).fillna(prior).astype(np.float64).values)\n",
    "\n",
    "        train_df[te_name] = train_te\n",
    "        test_df[te_name] = np.mean(np.vstack(test_te_folds), axis=0)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ---- ここがあなたのコードの差し替え部分 ----\n",
    "\n",
    "# TE対象を「object列のうち、条件を満たす列」に絞る\n",
    "TE_COLS_RAW = categories\n",
    "TE_COLS = select_te_cols(\n",
    "    train, test, TE_COLS_RAW,\n",
    "    min_unique=3,\n",
    "    max_unique_abs=5000,\n",
    "    max_unique_ratio=0.30,\n",
    "    max_missing=0.60,\n",
    "    rare_thr=5,\n",
    "    max_rare_points_ratio=0.80,\n",
    "    max_unseen_ratio=0.20\n",
    ")\n",
    "print(f\"Target Encoding applied to {len(TE_COLS)} features.\")\n",
    "print(\"TE_COLS:\", TE_COLS)\n",
    "\n",
    "# OOF TE作成\n",
    "train, test = add_target_encoding_oof(train, test, y, TE_COLS, n_splits=5, seed=42, smoothing=20)\n",
    "\n",
    "TE_FEATURES = [f\"te_{c}\" for c in TE_COLS]\n",
    "features = base + ORIG + TE_FEATURES\n",
    "\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "X_test = test[features].copy()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.columns)\n",
    "print(X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7e12f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage_safe(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == np.int64:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:8.83617\n",
      "[200]\tvalidation_0-rmse:8.79972\n",
      "[400]\tvalidation_0-rmse:8.78195\n",
      "[600]\tvalidation_0-rmse:8.77016\n",
      "[800]\tvalidation_0-rmse:8.76038\n",
      "[1000]\tvalidation_0-rmse:8.75169\n",
      "[1200]\tvalidation_0-rmse:8.74433\n",
      "[1400]\tvalidation_0-rmse:8.73851\n",
      "[1600]\tvalidation_0-rmse:8.73309\n",
      "[1800]\tvalidation_0-rmse:8.72833\n",
      "[2000]\tvalidation_0-rmse:8.72396\n",
      "[2200]\tvalidation_0-rmse:8.72059\n",
      "[2400]\tvalidation_0-rmse:8.71744\n",
      "[2600]\tvalidation_0-rmse:8.71456\n",
      "[2800]\tvalidation_0-rmse:8.71177\n",
      "[3000]\tvalidation_0-rmse:8.70922\n",
      "[3200]\tvalidation_0-rmse:8.70699\n",
      "[3400]\tvalidation_0-rmse:8.70481\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 非線形モデル例（どれか1つでOK）\n",
    "from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) 前提: X, y, X_test が既にできている想定\n",
    "#   X: train特徴量(DataFrame)\n",
    "#   y: target(Series)\n",
    "#   X_test: test特徴量(DataFrame)\n",
    "# =========================\n",
    "\n",
    "# カラム型を自動判定（object/category をカテゴリ扱い）\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 1段目（線形）と 2段目（残差）のOOFを作る\n",
    "oof_linear = np.zeros(len(X))\n",
    "oof_resid  = np.zeros(len(X))\n",
    "test_linear_folds = []\n",
    "test_resid_folds  = []\n",
    "\n",
    "# =========================\n",
    "# 1) 線形モデル（Ridge推奨）\n",
    "# =========================\n",
    "linear_model = Pipeline(steps=[\n",
    "    (\"pre\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "        ]\n",
    "    )),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 2) 残差モデル（XGB / LGBM / MLP）\n",
    "#   ここではXGB例（カテゴリは factorize して数値化）\n",
    "# =========================\n",
    "xgb_params = dict(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=200,\n",
    ")\n",
    "\n",
    "def factorize_fit_transform(train_df, val_df, test_df, cat_cols):\n",
    "    \"\"\"fold内でリークしないように、train+val+test を結合して factorize して整合を取る\"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    val_df   = val_df.copy()\n",
    "    test_df  = test_df.copy()\n",
    "\n",
    "    for c in cat_cols:\n",
    "        comb = pd.concat([train_df[c], val_df[c], test_df[c]], axis=0)\n",
    "        codes, _ = comb.factorize(sort=True)\n",
    "        n_tr = len(train_df)\n",
    "        n_va = len(val_df)\n",
    "\n",
    "        train_df[c] = codes[:n_tr]\n",
    "        val_df[c]   = codes[n_tr:n_tr+n_va]\n",
    "        test_df[c]  = codes[n_tr+n_va:]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) CV学習: 線形→残差→合成\n",
    "# =========================\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):\n",
    "    X_tr, y_tr = X.iloc[tr_idx].copy(), y.iloc[tr_idx].copy()\n",
    "    X_va, y_va = X.iloc[va_idx].copy(), y.iloc[va_idx].copy()\n",
    "\n",
    "    # ---- (A) 線形 fit -> pred ----\n",
    "    linear_model.fit(X_tr, y_tr)\n",
    "    pred_lin_va = linear_model.predict(X_va)\n",
    "    oof_linear[va_idx] = pred_lin_va\n",
    "\n",
    "    # testの線形pred（fold平均）\n",
    "    pred_lin_te = linear_model.predict(X_test)\n",
    "    test_linear_folds.append(pred_lin_te)\n",
    "\n",
    "    # ---- (B) 残差を作る ----\n",
    "    resid_tr = y_tr - linear_model.predict(X_tr)\n",
    "    resid_va_true = y_va - pred_lin_va  # 評価用\n",
    "\n",
    "    # ---- (C) 残差モデル fit -> pred ----\n",
    "    # XGB用にカテゴリを数値化（fold内で整合）\n",
    "    X_tr2, X_va2, X_te2 = factorize_fit_transform(X_tr, X_va, X_test, cat_cols)\n",
    "\n",
    "    X_tr2 = reduce_mem_usage_safe(X_tr2)\n",
    "    X_va2 = reduce_mem_usage_safe(X_va2)\n",
    "    X_te2 = reduce_mem_usage_safe(X_te2)\n",
    "\n",
    "    resid_model = XGBRegressor(**xgb_params)\n",
    "    resid_model.fit(\n",
    "        X_tr2, resid_tr,\n",
    "        eval_set=[(X_va2, resid_va_true)],\n",
    "        verbose=200\n",
    "    )\n",
    "\n",
    "    pred_resid_va = resid_model.predict(X_va2)\n",
    "    oof_resid[va_idx] = pred_resid_va\n",
    "\n",
    "    pred_resid_te = resid_model.predict(X_te2)\n",
    "    test_resid_folds.append(pred_resid_te)\n",
    "\n",
    "    # ---- (D) 合成 ----\n",
    "    pred_final_va = pred_lin_va + pred_resid_va\n",
    "    rmse = np.sqrt(mean_squared_error(y_va, pred_final_va))\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.5f}\")\n",
    "\n",
    "# 全体OOF\n",
    "oof_final = oof_linear + oof_resid\n",
    "print(\"-\"*30)\n",
    "print(f\"OOF RMSE: {np.sqrt(mean_squared_error(y, oof_final)):.5f}\")\n",
    "\n",
    "# test予測（fold平均）\n",
    "test_linear = np.mean(np.vstack(test_linear_folds), axis=0)\n",
    "test_resid  = np.mean(np.vstack(test_resid_folds), axis=0)\n",
    "test_pred   = test_linear + test_resid\n",
    "\n",
    "# 送信用（id列名は適宜）\n",
    "# sub = pd.DataFrame({\"id\": test[\"id\"], \"exam_score\": test_pred})\n",
    "# sub.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f89f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630000</td>\n",
       "      <td>70.958174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630001</td>\n",
       "      <td>69.800803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>630002</td>\n",
       "      <td>88.742019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>630003</td>\n",
       "      <td>55.147936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>630004</td>\n",
       "      <td>45.419446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  exam_score\n",
       "0  630000   70.958174\n",
       "1  630001   69.800803\n",
       "2  630002   88.742019\n",
       "3  630003   55.147936\n",
       "4  630004   45.419446"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission 作成（id列は自動で推定して作る）\n",
    "pred_col = target  # 'exam_score'\n",
    "\n",
    "# id列名を推定\n",
    "id_col = \"id\" if \"id\" in test.columns else (\"Id\" if \"Id\" in test.columns else None)\n",
    "if id_col is None:\n",
    "    raise ValueError(\"testに id / Id 列が見つかりません。id列名を手動で指定してください。\")\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    id_col: test[id_col].values,\n",
    "    pred_col: test_pred  # 直前で作った最終予測（線形 + 残差）\n",
    "})\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
