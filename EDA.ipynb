{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212db142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5526e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape: (630000, 13)\n",
      "test.shape: (270000, 12)\n",
      "orig.shape: (20000, 13)\n",
      "11 Base Features:['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, gc\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "orig = pd.read_csv('Exam_Score_Prediction.csv')\n",
    "\n",
    "print(\"train_shape:\",train.shape)\n",
    "print(\"test.shape:\",test.shape)\n",
    "print(\"orig.shape:\",orig.shape)\n",
    "\n",
    "orig\n",
    "\n",
    "# 今後のためにリストを作る\n",
    "target = 'exam_score'\n",
    "base = [col for col in train.columns if col not in ['id', target]]\n",
    "categories = train.select_dtypes('object').columns.to_list()\n",
    "nums = [col for col in base if col not in categories]\n",
    "print(f'{len(base)} Base Features:{base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5970102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ORIG Features Created.\n"
     ]
    }
   ],
   "source": [
    "ORIG = []\n",
    "\n",
    "# 外部データの各カラムのユニークごとの平均値というカラムを追加する。\n",
    "for col in base:\n",
    "    # 一つの列に対してgroupbyで固有の値をまとめる。それらのtargetをそれぞれ平均する\n",
    "    mean_map = orig.groupby(col)[target].mean() \n",
    "    new_mean_col_name = f\"orig_mean_{col}\"\n",
    "    mean_map.name = new_mean_col_name\n",
    "    \n",
    "    train = train.merge(mean_map, on=col, how='left') # colをキーにして\n",
    "    test = test.merge(mean_map, on=col, how='left')\n",
    "    ORIG.append(new_mean_col_name)\n",
    "    \n",
    "# 外部データの各カラムのユニークごとのサイズというカラムを追加する。\n",
    "    new_count_col_name = f\"orig_count_{col}\"\n",
    "    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n",
    "    \n",
    "    train = train.merge(count_map, on=col, how='left')\n",
    "    test = test.merge(count_map, on=col, how='left')\n",
    "    ORIG.append(new_count_col_name)\n",
    "\n",
    "print(f'{len(ORIG)} ORIG Features Created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8490d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origには存在するが、trainには存在しないカテゴリを全体平均で埋める\n",
    "for col in ORIG:\n",
    "    if 'mean' in col:\n",
    "        train[col] = train[col].fillna(orig[target].mean())\n",
    "        test[col] = test[col].fillna(orig[target].mean())\n",
    "    else:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3406c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_mem_usage はここに定義（そのままでOK）\n",
    "\n",
    "features = base + ORIG\n",
    "\n",
    "# まず X, y を作る（これが先）\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "\n",
    "# test側も、モデルに入れる列だけにそろえる（重要）\n",
    "X_test = test[features].copy()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7f5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding applied to 6 features.\n",
      "TE_COLS: ['gender', 'course', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n",
      "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance',\n",
      "       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n",
      "       'facility_rating', 'exam_difficulty', 'orig_mean_age', 'orig_count_age',\n",
      "       'orig_mean_gender', 'orig_count_gender', 'orig_mean_course',\n",
      "       'orig_count_course', 'orig_mean_study_hours', 'orig_count_study_hours',\n",
      "       'orig_mean_class_attendance', 'orig_count_class_attendance',\n",
      "       'orig_mean_internet_access', 'orig_count_internet_access',\n",
      "       'orig_mean_sleep_hours', 'orig_count_sleep_hours',\n",
      "       'orig_mean_sleep_quality', 'orig_count_sleep_quality',\n",
      "       'orig_mean_study_method', 'orig_count_study_method',\n",
      "       'orig_mean_facility_rating', 'orig_count_facility_rating',\n",
      "       'orig_mean_exam_difficulty', 'orig_count_exam_difficulty', 'te_gender',\n",
      "       'te_course', 'te_sleep_quality', 'te_study_method',\n",
      "       'te_facility_rating', 'te_exam_difficulty'],\n",
      "      dtype='object')\n",
      "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance',\n",
      "       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n",
      "       'facility_rating', 'exam_difficulty', 'orig_mean_age', 'orig_count_age',\n",
      "       'orig_mean_gender', 'orig_count_gender', 'orig_mean_course',\n",
      "       'orig_count_course', 'orig_mean_study_hours', 'orig_count_study_hours',\n",
      "       'orig_mean_class_attendance', 'orig_count_class_attendance',\n",
      "       'orig_mean_internet_access', 'orig_count_internet_access',\n",
      "       'orig_mean_sleep_hours', 'orig_count_sleep_hours',\n",
      "       'orig_mean_sleep_quality', 'orig_count_sleep_quality',\n",
      "       'orig_mean_study_method', 'orig_count_study_method',\n",
      "       'orig_mean_facility_rating', 'orig_count_facility_rating',\n",
      "       'orig_mean_exam_difficulty', 'orig_count_exam_difficulty', 'te_gender',\n",
      "       'te_course', 'te_sleep_quality', 'te_study_method',\n",
      "       'te_facility_rating', 'te_exam_difficulty'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Target Encoding (OOFでリーク防止) + 列選別条件つき\n",
    "# =========================\n",
    "\n",
    "def select_te_cols(\n",
    "    df_train, df_test, cols,\n",
    "    min_unique=3,              # unique <=2 は除外\n",
    "    max_unique_abs=5000,       # 高カーディナリティ除外\n",
    "    max_unique_ratio=0.30,     # unique/行数 が大きすぎる列は除外（ID化）\n",
    "    max_missing=0.60,          # 欠損率が高い列は除外\n",
    "    rare_thr=5,                # レア判定（出現回数<=5）\n",
    "    max_rare_points_ratio=0.80,# レアカテゴリが占める割合が大きい列は除外\n",
    "    max_unseen_ratio=0.20      # testにしかないカテゴリが多い列は除外\n",
    "):\n",
    "    n = len(df_train)\n",
    "    chosen = []\n",
    "    for col in cols:\n",
    "        s_tr = df_train[col]\n",
    "        s_te = df_test[col]\n",
    "\n",
    "        # 欠損\n",
    "        if s_tr.isna().mean() > max_missing:\n",
    "            continue\n",
    "\n",
    "        # unique\n",
    "        nunq = s_tr.nunique(dropna=True)\n",
    "        if nunq < min_unique:\n",
    "            continue\n",
    "        if nunq > max_unique_abs:\n",
    "            continue\n",
    "        if nunq / n > max_unique_ratio:\n",
    "            continue\n",
    "\n",
    "        # レアカテゴリ比率\n",
    "        vc = s_tr.value_counts(dropna=True)\n",
    "        rare_points_ratio = (s_tr.map(vc).fillna(0) <= rare_thr).mean()\n",
    "        if rare_points_ratio > max_rare_points_ratio:\n",
    "            continue\n",
    "\n",
    "        # unseen比率（testにあるがtrainにないカテゴリの比率）\n",
    "        tr_set = set(s_tr.dropna().unique())\n",
    "        te_set = set(s_te.dropna().unique())\n",
    "        if len(te_set) > 0:\n",
    "            unseen_ratio = len(te_set - tr_set) / len(te_set)\n",
    "            if unseen_ratio > max_unseen_ratio:\n",
    "                continue\n",
    "\n",
    "        chosen.append(col)\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def add_target_encoding_oof(train_df, test_df, y, te_cols, n_splits=5, seed=42, smoothing=20):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for col in te_cols:\n",
    "        te_name = f\"te_{col}\"\n",
    "        train_te = np.zeros(len(train_df), dtype=np.float64)\n",
    "        test_te_folds = []\n",
    "\n",
    "        for tr_idx, va_idx in kf.split(train_df):\n",
    "            X_tr = train_df.iloc[tr_idx]\n",
    "            y_tr = y.iloc[tr_idx]\n",
    "            X_va = train_df.iloc[va_idx]\n",
    "\n",
    "            prior = y_tr.mean()\n",
    "\n",
    "            stats = (\n",
    "                pd.DataFrame({col: X_tr[col].values, \"y\": y_tr.values})\n",
    "                .groupby(col)[\"y\"]\n",
    "                .agg([\"mean\", \"count\"])\n",
    "            )\n",
    "\n",
    "            smooth_map = (stats[\"count\"] * stats[\"mean\"] + smoothing * prior) / (stats[\"count\"] + smoothing)\n",
    "\n",
    "            train_te[va_idx] = X_va[col].map(smooth_map).fillna(prior).astype(np.float64).values\n",
    "            test_te_folds.append(test_df[col].map(smooth_map).fillna(prior).astype(np.float64).values)\n",
    "\n",
    "        train_df[te_name] = train_te\n",
    "        test_df[te_name] = np.mean(np.vstack(test_te_folds), axis=0)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ---- ここがあなたのコードの差し替え部分 ----\n",
    "\n",
    "# TE対象を「object列のうち、条件を満たす列」に絞る\n",
    "TE_COLS_RAW = categories\n",
    "TE_COLS = select_te_cols(\n",
    "    train, test, TE_COLS_RAW,\n",
    "    min_unique=3,\n",
    "    max_unique_abs=5000,\n",
    "    max_unique_ratio=0.30,\n",
    "    max_missing=0.60,\n",
    "    rare_thr=5,\n",
    "    max_rare_points_ratio=0.80,\n",
    "    max_unseen_ratio=0.20\n",
    ")\n",
    "print(f\"Target Encoding applied to {len(TE_COLS)} features.\")\n",
    "print(\"TE_COLS:\", TE_COLS)\n",
    "\n",
    "# OOF TE作成\n",
    "train, test = add_target_encoding_oof(train, test, y, TE_COLS, n_splits=5, seed=42, smoothing=20)\n",
    "\n",
    "TE_FEATURES = [f\"te_{c}\" for c in TE_COLS]\n",
    "features = base + ORIG + TE_FEATURES\n",
    "\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "X_test = test[features].copy()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.columns)\n",
    "print(X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e12f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage_safe(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == np.int64:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91250702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_lin: (630000, 58)\n",
      "X_xgb: (630000, 58)\n",
      "y: (630000,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# one-hot（fold外で1回）\n",
    "# =========================\n",
    "\n",
    "# 線形モデル用\n",
    "X_lin = pd.get_dummies(train[features], drop_first=False)\n",
    "test_lin_X = pd.get_dummies(test[features], drop_first=False)\n",
    "X_lin, test_lin_X = X_lin.align(test_lin_X, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "# 残差XGB用（同じでOKだが分けておく）\n",
    "X_xgb = pd.get_dummies(train[features], drop_first=False)\n",
    "test_xgb = pd.get_dummies(test[features], drop_first=False)\n",
    "X_xgb, test_xgb = X_xgb.align(test_xgb, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "y = train[target]\n",
    "\n",
    "print(\"X_lin:\", X_lin.shape)\n",
    "print(\"X_xgb:\", X_xgb.shape)\n",
    "print(\"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1d49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ridge] Fold 1 | RMSE Model1: 8.67853 | RMSE Model2(pseudo,w): 8.68340\n",
      "[Ridge] Fold 2 | RMSE Model1: 8.68095 | RMSE Model2(pseudo,w): 8.68183\n",
      "[Ridge] Fold 3 | RMSE Model1: 8.67886 | RMSE Model2(pseudo,w): 8.68156\n",
      "[Ridge] Fold 4 | RMSE Model1: 8.69098 | RMSE Model2(pseudo,w): 8.69702\n",
      "[Ridge] Fold 5 | RMSE Model1: 8.70321 | RMSE Model2(pseudo,w): 8.71120\n",
      "[Ridge] OOF RMSE Model1: 8.68651\n",
      "[Ridge] OOF RMSE Model2(pseudo,w): 8.69101\n",
      "       id  exam_score\n",
      "0  630000   70.640470\n",
      "1  630001   70.106281\n",
      "2  630002   89.464373\n",
      "3  630003   55.330491\n",
      "4  630004   45.911533\n",
      "saved: submission_Ridge_pseudo_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 重み付き Pseudo Label 版（xgb.train / 古いxgboost対応）\n",
    "# 線形モデル + 残差XGB\n",
    "# - Model1: train真残差で学習 → val/test残差を予測（擬似ラベル）\n",
    "# - Model2: train(真) + val/test(擬似) で学習（擬似には小さめ重み）\n",
    "# - early stopping は train fold の真データから切った holdout のみで実施（leak回避）\n",
    "# CV → test予測 → submission 作成まで全部\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 前提:\n",
    "# X_lin, X_xgb, test_lin_X, test_xgb, y, test が既に定義済み\n",
    "# =========================\n",
    "\n",
    "MODEL_NAME = \"Ridge\"  # Ridge / Lasso / ElasticNet / Huber\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "# 擬似ラベル重み（ここが主な調整ノブ）\n",
    "W_PSEUDO_VAL  = 0.3\n",
    "W_PSEUDO_TEST = 0.3\n",
    "\n",
    "# early stopping 用 holdout（真データからのみ）\n",
    "ES_FRAC = 0.10\n",
    "\n",
    "def make_linear(name, seed):\n",
    "    if name == \"Ridge\":\n",
    "        return Ridge(alpha=1.0, random_state=seed)\n",
    "    if name == \"Lasso\":\n",
    "        return Lasso(alpha=1e-3, random_state=seed, max_iter=5000)\n",
    "    if name == \"ElasticNet\":\n",
    "        return ElasticNet(alpha=1e-3, l1_ratio=0.5, random_state=seed, max_iter=5000)\n",
    "    if name == \"Huber\":\n",
    "        return HuberRegressor(alpha=1e-4, epsilon=1.35, max_iter=1000)\n",
    "    raise ValueError(\"invalid MODEL_NAME\")\n",
    "\n",
    "def dm(X, y=None, w=None):\n",
    "    d = xgb.DMatrix(X, label=y) if y is not None else xgb.DMatrix(X)\n",
    "    if w is not None:\n",
    "        d.set_weight(w)\n",
    "    return d\n",
    "\n",
    "# xgb.train params\n",
    "params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eta=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    seed=SEED,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "\n",
    "NUM_BOOST_ROUND = 20000\n",
    "EARLY_STOP = 200\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_m1 = np.zeros(len(X_lin))\n",
    "oof_m2 = np.zeros(len(X_lin))\n",
    "test_pred_folds_m2 = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_lin), 1):\n",
    "    # split\n",
    "    X_tr_lin, X_va_lin = X_lin.iloc[tr_idx], X_lin.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    X_tr_xgb, X_va_xgb = X_xgb.iloc[tr_idx], X_xgb.iloc[va_idx]\n",
    "\n",
    "    # 1) 線形\n",
    "    lin = make_linear(MODEL_NAME, SEED)\n",
    "    lin.fit(X_tr_lin, y_tr)\n",
    "\n",
    "    tr_lin = lin.predict(X_tr_lin)\n",
    "    va_lin = lin.predict(X_va_lin)\n",
    "    te_lin = lin.predict(test_lin_X)\n",
    "\n",
    "    # 2) 残差（真）\n",
    "    y_tr_res = (y_tr.values - tr_lin)\n",
    "    y_va_res_true = (y_va.values - va_lin)\n",
    "\n",
    "    # -------------------------\n",
    "    # Model1: 残差XGB（真の残差で学習）\n",
    "    # -------------------------\n",
    "    dtr1 = dm(X_tr_xgb, y_tr_res)\n",
    "    dva1 = dm(X_va_xgb, y_va_res_true)\n",
    "    dte  = dm(test_xgb)\n",
    "\n",
    "    bst1 = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtr1,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        evals=[(dtr1, \"train\"), (dva1, \"valid\")],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    va_res_m1 = bst1.predict(dva1, iteration_range=(0, bst1.best_iteration + 1))\n",
    "    te_res_m1 = bst1.predict(dte,  iteration_range=(0, bst1.best_iteration + 1))\n",
    "\n",
    "    va_pred_m1 = va_lin + va_res_m1\n",
    "    oof_m1[va_idx] = va_pred_m1\n",
    "    rmse1 = np.sqrt(mean_squared_error(y_va, va_pred_m1))\n",
    "\n",
    "    # -------------------------\n",
    "    # 擬似ラベル（残差）\n",
    "    # -------------------------\n",
    "    y_va_res_pseudo = va_res_m1\n",
    "    y_te_res_pseudo = te_res_m1\n",
    "\n",
    "    # -------------------------\n",
    "    # Model2: 真(train) + 擬似(val/test) を重み付きで学習\n",
    "    # early stopping は train真データから切った holdout のみ\n",
    "    # -------------------------\n",
    "    idx_all = np.arange(len(tr_idx))\n",
    "    idx_fit, idx_es = train_test_split(\n",
    "        idx_all, test_size=ES_FRAC, random_state=SEED + fold, shuffle=True\n",
    "    )\n",
    "\n",
    "    # fit 用（真 train の一部 + val擬似 + test擬似）\n",
    "    X_fit_true = X_tr_xgb.iloc[idx_fit]\n",
    "    y_fit_true = y_tr_res[idx_fit]\n",
    "    w_fit_true = np.ones(len(idx_fit), dtype=np.float32)\n",
    "\n",
    "    X_fit_all = pd.concat([X_fit_true, X_va_xgb, test_xgb], axis=0, ignore_index=True)\n",
    "    y_fit_all = np.concatenate([y_fit_true, y_va_res_pseudo, y_te_res_pseudo], axis=0)\n",
    "\n",
    "    w_fit_all = np.concatenate([\n",
    "        w_fit_true,\n",
    "        np.full(len(X_va_xgb),  W_PSEUDO_VAL,  dtype=np.float32),\n",
    "        np.full(len(test_xgb),  W_PSEUDO_TEST, dtype=np.float32),\n",
    "    ], axis=0)\n",
    "\n",
    "    # early stop 用（真 train の holdout）\n",
    "    X_es_true = X_tr_xgb.iloc[idx_es]\n",
    "    y_es_true = y_tr_res[idx_es]\n",
    "    w_es_true = np.ones(len(idx_es), dtype=np.float32)\n",
    "\n",
    "    dfit2 = dm(X_fit_all, y_fit_all, w_fit_all)\n",
    "    des2  = dm(X_es_true, y_es_true, w_es_true)\n",
    "\n",
    "    bst2 = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dfit2,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        evals=[(dfit2, \"train\"), (des2, \"valid\")],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    va_res_m2 = bst2.predict(dva1, iteration_range=(0, bst2.best_iteration + 1))\n",
    "    te_res_m2 = bst2.predict(dte,  iteration_range=(0, bst2.best_iteration + 1))\n",
    "\n",
    "    va_pred_m2 = va_lin + va_res_m2\n",
    "    oof_m2[va_idx] = va_pred_m2\n",
    "\n",
    "    te_pred_m2 = te_lin + te_res_m2\n",
    "    test_pred_folds_m2.append(te_pred_m2)\n",
    "\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_va, va_pred_m2))\n",
    "\n",
    "    print(f\"[{MODEL_NAME}] Fold {fold} | RMSE Model1: {rmse1:.5f} | RMSE Model2(pseudo,w): {rmse2:.5f}\")\n",
    "\n",
    "# OOF\n",
    "rmse_oof_m1 = np.sqrt(mean_squared_error(y, oof_m1))\n",
    "rmse_oof_m2 = np.sqrt(mean_squared_error(y, oof_m2))\n",
    "print(f\"[{MODEL_NAME}] OOF RMSE Model1: {rmse_oof_m1:.5f}\")\n",
    "print(f\"[{MODEL_NAME}] OOF RMSE Model2(pseudo,w): {rmse_oof_m2:.5f}\")\n",
    "\n",
    "# test（fold平均）\n",
    "test_pred = np.mean(np.vstack(test_pred_folds_m2), axis=0)\n",
    "test_pred = np.clip(test_pred, y.min(), y.max())\n",
    "\n",
    "# submission\n",
    "sub = pd.DataFrame({\"id\": test[\"id\"], \"exam_score\": test_pred})\n",
    "out_path = f\"submission_{MODEL_NAME}_pseudo_weighted.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(sub.head())\n",
    "print(f\"saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
