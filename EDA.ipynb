{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "212db142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5526e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape: (630000, 13)\n",
      "test.shape: (270000, 12)\n",
      "orig.shape: (20000, 13)\n",
      "11 Base Features:['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, gc\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "orig = pd.read_csv('Exam_Score_Prediction.csv')\n",
    "\n",
    "print(\"train_shape:\",train.shape)\n",
    "print(\"test.shape:\",test.shape)\n",
    "print(\"orig.shape:\",orig.shape)\n",
    "\n",
    "orig\n",
    "\n",
    "# 今後のためにリストを作る\n",
    "target = 'exam_score'\n",
    "base = [col for col in train.columns if col not in ['id', target]]\n",
    "categories = train.select_dtypes('object').columns.to_list()\n",
    "nums = [col for col in base if col not in categories]\n",
    "print(f'{len(base)} Base Features:{base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5970102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ORIG Features Created.\n"
     ]
    }
   ],
   "source": [
    "ORIG = []\n",
    "\n",
    "# 外部データの各カラムのユニークごとの平均値というカラムを追加する。\n",
    "for col in base:\n",
    "    # 一つの列に対してgroupbyで固有の値をまとめる。それらのtargetをそれぞれ平均する\n",
    "    mean_map = orig.groupby(col)[target].mean() \n",
    "    new_mean_col_name = f\"orig_mean_{col}\"\n",
    "    mean_map.name = new_mean_col_name\n",
    "    \n",
    "    train = train.merge(mean_map, on=col, how='left') # colをキーにして\n",
    "    test = test.merge(mean_map, on=col, how='left')\n",
    "    ORIG.append(new_mean_col_name)\n",
    "    \n",
    "# 外部データの各カラムのユニークごとのサイズというカラムを追加する。\n",
    "    new_count_col_name = f\"orig_count_{col}\"\n",
    "    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n",
    "    \n",
    "    train = train.merge(count_map, on=col, how='left')\n",
    "    test = test.merge(count_map, on=col, how='left')\n",
    "    ORIG.append(new_count_col_name)\n",
    "\n",
    "print(f'{len(ORIG)} ORIG Features Created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8490d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origには存在するが、trainには存在しないカテゴリを全体平均で埋める\n",
    "for col in ORIG:\n",
    "    if 'mean' in col:\n",
    "        train[col] = train[col].fillna(orig[target].mean())\n",
    "        test[col] = test[col].fillna(orig[target].mean())\n",
    "    else:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3406c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_mem_usage はここに定義（そのままでOK）\n",
    "\n",
    "features = base + ORIG\n",
    "\n",
    "# まず X, y を作る（これが先）\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "\n",
    "# test側も、モデルに入れる列だけにそろえる（重要）\n",
    "X_test = test[features].copy()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba7f5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding applied to 6 features.\n",
      "TE_COLS: ['gender', 'course', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n",
      "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance',\n",
      "       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n",
      "       'facility_rating', 'exam_difficulty', 'orig_mean_age', 'orig_count_age',\n",
      "       'orig_mean_gender', 'orig_count_gender', 'orig_mean_course',\n",
      "       'orig_count_course', 'orig_mean_study_hours', 'orig_count_study_hours',\n",
      "       'orig_mean_class_attendance', 'orig_count_class_attendance',\n",
      "       'orig_mean_internet_access', 'orig_count_internet_access',\n",
      "       'orig_mean_sleep_hours', 'orig_count_sleep_hours',\n",
      "       'orig_mean_sleep_quality', 'orig_count_sleep_quality',\n",
      "       'orig_mean_study_method', 'orig_count_study_method',\n",
      "       'orig_mean_facility_rating', 'orig_count_facility_rating',\n",
      "       'orig_mean_exam_difficulty', 'orig_count_exam_difficulty', 'te_gender',\n",
      "       'te_course', 'te_sleep_quality', 'te_study_method',\n",
      "       'te_facility_rating', 'te_exam_difficulty'],\n",
      "      dtype='object')\n",
      "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance',\n",
      "       'internet_access', 'sleep_hours', 'sleep_quality', 'study_method',\n",
      "       'facility_rating', 'exam_difficulty', 'orig_mean_age', 'orig_count_age',\n",
      "       'orig_mean_gender', 'orig_count_gender', 'orig_mean_course',\n",
      "       'orig_count_course', 'orig_mean_study_hours', 'orig_count_study_hours',\n",
      "       'orig_mean_class_attendance', 'orig_count_class_attendance',\n",
      "       'orig_mean_internet_access', 'orig_count_internet_access',\n",
      "       'orig_mean_sleep_hours', 'orig_count_sleep_hours',\n",
      "       'orig_mean_sleep_quality', 'orig_count_sleep_quality',\n",
      "       'orig_mean_study_method', 'orig_count_study_method',\n",
      "       'orig_mean_facility_rating', 'orig_count_facility_rating',\n",
      "       'orig_mean_exam_difficulty', 'orig_count_exam_difficulty', 'te_gender',\n",
      "       'te_course', 'te_sleep_quality', 'te_study_method',\n",
      "       'te_facility_rating', 'te_exam_difficulty'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Target Encoding (OOFでリーク防止) + 列選別条件つき\n",
    "# =========================\n",
    "\n",
    "def select_te_cols(\n",
    "    df_train, df_test, cols,\n",
    "    min_unique=3,              # unique <=2 は除外\n",
    "    max_unique_abs=5000,       # 高カーディナリティ除外\n",
    "    max_unique_ratio=0.30,     # unique/行数 が大きすぎる列は除外（ID化）\n",
    "    max_missing=0.60,          # 欠損率が高い列は除外\n",
    "    rare_thr=5,                # レア判定（出現回数<=5）\n",
    "    max_rare_points_ratio=0.80,# レアカテゴリが占める割合が大きい列は除外\n",
    "    max_unseen_ratio=0.20      # testにしかないカテゴリが多い列は除外\n",
    "):\n",
    "    n = len(df_train)\n",
    "    chosen = []\n",
    "    for col in cols:\n",
    "        s_tr = df_train[col]\n",
    "        s_te = df_test[col]\n",
    "\n",
    "        # 欠損\n",
    "        if s_tr.isna().mean() > max_missing:\n",
    "            continue\n",
    "\n",
    "        # unique\n",
    "        nunq = s_tr.nunique(dropna=True)\n",
    "        if nunq < min_unique:\n",
    "            continue\n",
    "        if nunq > max_unique_abs:\n",
    "            continue\n",
    "        if nunq / n > max_unique_ratio:\n",
    "            continue\n",
    "\n",
    "        # レアカテゴリ比率\n",
    "        vc = s_tr.value_counts(dropna=True)\n",
    "        rare_points_ratio = (s_tr.map(vc).fillna(0) <= rare_thr).mean()\n",
    "        if rare_points_ratio > max_rare_points_ratio:\n",
    "            continue\n",
    "\n",
    "        # unseen比率（testにあるがtrainにないカテゴリの比率）\n",
    "        tr_set = set(s_tr.dropna().unique())\n",
    "        te_set = set(s_te.dropna().unique())\n",
    "        if len(te_set) > 0:\n",
    "            unseen_ratio = len(te_set - tr_set) / len(te_set)\n",
    "            if unseen_ratio > max_unseen_ratio:\n",
    "                continue\n",
    "\n",
    "        chosen.append(col)\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def add_target_encoding_oof(train_df, test_df, y, te_cols, n_splits=5, seed=42, smoothing=20):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for col in te_cols:\n",
    "        te_name = f\"te_{col}\"\n",
    "        train_te = np.zeros(len(train_df), dtype=np.float64)\n",
    "        test_te_folds = []\n",
    "\n",
    "        for tr_idx, va_idx in kf.split(train_df):\n",
    "            X_tr = train_df.iloc[tr_idx]\n",
    "            y_tr = y.iloc[tr_idx]\n",
    "            X_va = train_df.iloc[va_idx]\n",
    "\n",
    "            prior = y_tr.mean()\n",
    "\n",
    "            stats = (\n",
    "                pd.DataFrame({col: X_tr[col].values, \"y\": y_tr.values})\n",
    "                .groupby(col)[\"y\"]\n",
    "                .agg([\"mean\", \"count\"])\n",
    "            )\n",
    "\n",
    "            smooth_map = (stats[\"count\"] * stats[\"mean\"] + smoothing * prior) / (stats[\"count\"] + smoothing)\n",
    "\n",
    "            train_te[va_idx] = X_va[col].map(smooth_map).fillna(prior).astype(np.float64).values\n",
    "            test_te_folds.append(test_df[col].map(smooth_map).fillna(prior).astype(np.float64).values)\n",
    "\n",
    "        train_df[te_name] = train_te\n",
    "        test_df[te_name] = np.mean(np.vstack(test_te_folds), axis=0)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ---- ここがあなたのコードの差し替え部分 ----\n",
    "\n",
    "# TE対象を「object列のうち、条件を満たす列」に絞る\n",
    "TE_COLS_RAW = categories\n",
    "TE_COLS = select_te_cols(\n",
    "    train, test, TE_COLS_RAW,\n",
    "    min_unique=3,\n",
    "    max_unique_abs=5000,\n",
    "    max_unique_ratio=0.30,\n",
    "    max_missing=0.60,\n",
    "    rare_thr=5,\n",
    "    max_rare_points_ratio=0.80,\n",
    "    max_unseen_ratio=0.20\n",
    ")\n",
    "print(f\"Target Encoding applied to {len(TE_COLS)} features.\")\n",
    "print(\"TE_COLS:\", TE_COLS)\n",
    "\n",
    "# OOF TE作成\n",
    "train, test = add_target_encoding_oof(train, test, y, TE_COLS, n_splits=5, seed=42, smoothing=20)\n",
    "\n",
    "TE_FEATURES = [f\"te_{c}\" for c in TE_COLS]\n",
    "features = base + ORIG + TE_FEATURES\n",
    "\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "X_test = test[features].copy()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.columns)\n",
    "print(X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a7e12f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage_safe(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == np.int64:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d61f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 RMSE: 8.71649\n",
      "Fold 2 RMSE: 8.71670\n",
      "Fold 3 RMSE: 8.71520\n",
      "Fold 4 RMSE: 8.72429\n",
      "Fold 5 RMSE: 8.72759\n",
      "OOF RMSE: 8.720055446187217\n",
      "       id  exam_score\n",
      "0  630000   69.578297\n",
      "1  630001   69.974591\n",
      "2  630002   90.840441\n",
      "3  630003   55.646008\n",
      "4  630004   46.875301\n",
      "saved: submission.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 1) one-hot（fold外で1回）\n",
    "# =========================\n",
    "\n",
    "# Ridge 用\n",
    "X_lin = pd.get_dummies(train[features], drop_first=False)\n",
    "test_lin_X = pd.get_dummies(test[features], drop_first=False)\n",
    "X_lin, test_lin_X = X_lin.align(test_lin_X, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "# XGB 用\n",
    "X_xgb = pd.get_dummies(train[features], drop_first=False)\n",
    "test_xgb = pd.get_dummies(test[features], drop_first=False)\n",
    "X_xgb, test_xgb = X_xgb.align(test_xgb, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "y = train[target]\n",
    "\n",
    "# =========================\n",
    "# 2) CV 設定\n",
    "# =========================\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_lin = np.zeros(len(train))\n",
    "oof_res = np.zeros(len(train))\n",
    "test_lin_pred = np.zeros(len(test))\n",
    "test_res_pred = np.zeros(len(test))\n",
    "\n",
    "# =========================\n",
    "# 3) CV ループ\n",
    "# =========================\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_lin)):\n",
    "\n",
    "    # ----- Ridge -----\n",
    "    X_tr_lin, X_va_lin = X_lin.iloc[tr_idx], X_lin.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    lin = Ridge(alpha=1.0, random_state=42)\n",
    "    lin.fit(X_tr_lin, y_tr)\n",
    "\n",
    "    y_tr_lin = lin.predict(X_tr_lin)\n",
    "    y_va_lin = lin.predict(X_va_lin)\n",
    "\n",
    "    oof_lin[va_idx] = y_va_lin\n",
    "    test_lin_pred += lin.predict(test_lin_X) / N_SPLITS\n",
    "\n",
    "    # ----- 残差 -----\n",
    "    y_tr_res = y_tr - y_tr_lin\n",
    "    y_va_res = y_va - y_va_lin\n",
    "\n",
    "    # ----- XGB（残差） -----\n",
    "    X_tr_xgb, X_va_xgb = X_xgb.iloc[tr_idx], X_xgb.iloc[va_idx]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=12000,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(\n",
    "        X_tr_xgb, y_tr_res,\n",
    "        eval_set=[(X_va_xgb, y_va_res)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    y_va_res_pred = xgb_model.predict(X_va_xgb)\n",
    "    oof_res[va_idx] = y_va_res_pred\n",
    "    test_res_pred += xgb_model.predict(test_xgb) / N_SPLITS\n",
    "\n",
    "    # ----- Fold RMSE -----\n",
    "    fold_rmse = np.sqrt(\n",
    "        mean_squared_error(y_va, y_va_lin + y_va_res_pred)\n",
    "    )\n",
    "    print(f\"Fold {fold+1} RMSE: {fold_rmse:.5f}\")\n",
    "\n",
    "# =========================\n",
    "# 4) OOF 評価\n",
    "# =========================\n",
    "oof_pred = oof_lin + oof_res\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, oof_pred))\n",
    "print(\"OOF RMSE:\", cv_rmse)\n",
    "\n",
    "# =========================\n",
    "# 5) Test 予測\n",
    "# =========================\n",
    "test_pred = test_lin_pred + test_res_pred\n",
    "\n",
    "# =========================\n",
    "# 6) submission 作成\n",
    "# =========================\n",
    "# 予測値を範囲に収めたい場合（任意）\n",
    "test_pred = np.clip(test_pred, y.min(), y.max())\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"exam_score\": test_pred\n",
    "})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(sub.head())\n",
    "print(\"saved: submission.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
